{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AI Optix Intelligent profiling and optimization engine for AI workloads. AI Optix is a research-grade infrastructure tool designed to bridge the gap between high-level Python AI frameworks and low-level hardware performance. It prioritizes correctness , reproducibility , and transparency over \"black box\" auto-tuning. \ud83d\ude80 Key Features Hybrid Architecture : Python orchestration with Rust system agents and C++ high-performance math kernels. Hardware-Aware Benchmarking : Automatic detection of CPU/GPU/Accelerator capabilities (via MEBS). Correctness First : Rigorous validation of optimization results against PyTorch/NumPy baselines. System-Wide Profiling : Correlates model metrics with OS-level counters (context switches, cache misses). Safe Fallbacks : Guaranteed execution on CPU if specialized hardware is unavailable. \ud83c\udfd7 Architecture Overview Component Language Responsibility Core API Python User interface, experiment orchestration, visualization. System Agent Rust Safe threading, OS metric collection, background daemons. Kernels C++ SIMD-optimized math operations, hardware-specific acceleration. Device Abstraction C Low-level hardware query interface. \ud83d\udcbb Supported Platforms Linux (Primary, extensive support) macOS (Apple Silicon support via MPS) Windows (Experimental via WSL2) \ud83d\udcda Documentation Architecture Overview : High-level system design. CPU Execution : Deep dive into memory & threading. GPU Comparison : When to use ai_optix vs. PyTorch for GPU. Performance Notes : Bottlenecks and optimization guide. \u26a1 Quick Start Installation (Pip) We provide an automated setup script that handles: - Python 3.11 Virtual Environment creation - CPU vs CUDA Runtime detection - PyTorch version selection # Clone the repository git clone https://github.com/ai-foundation-software/ai-optix.git cd ai-optix # Run the setup script ./scripts/setup.sh # Activate the environment source .venv/bin/activate Running a Benchmark # Run the Reference MatMul Benchmark python -m ai_optix.benchmarks.mebs.matmul_benchmark Python API Example from ai_optix.benchmarks.mebs import BenchmarkRunner, BenchmarkConfig import torch def my_model_op(): # Your model code here pass config = BenchmarkConfig(name=\"MyModel\", warmup_iters=10, measure_iters=50) runner = BenchmarkRunner(config) runner.run(my_model_op) \ud83d\udee1 Safety Philosophy AI Optix adheres to strict principles: 1. Never optimize without measuring. 2. Never sacrifice precision for speed (unless explicitly requested via quantization levels). 3. Always report the baseline. \ud83d\uddfa Roadmap [x] Initial Hybrid Architecture (Python/Rust/C++) [x] Model Efficiency Benchmark Suite (MEBS) [ ] Automated Kernel Tuning [ ] Distributed Training Profiling [ ] Web-based Visualization Dashboard \ud83e\udd1d Contributing We welcome contributions! Please see CONTRIBUTING.md for details on our code style and development workflow. \ud83d\udcc4 License This project is licensed under the MIT License - see the LICENSE file for details.","title":"Home"},{"location":"#ai-optix","text":"Intelligent profiling and optimization engine for AI workloads. AI Optix is a research-grade infrastructure tool designed to bridge the gap between high-level Python AI frameworks and low-level hardware performance. It prioritizes correctness , reproducibility , and transparency over \"black box\" auto-tuning.","title":"AI Optix"},{"location":"#key-features","text":"Hybrid Architecture : Python orchestration with Rust system agents and C++ high-performance math kernels. Hardware-Aware Benchmarking : Automatic detection of CPU/GPU/Accelerator capabilities (via MEBS). Correctness First : Rigorous validation of optimization results against PyTorch/NumPy baselines. System-Wide Profiling : Correlates model metrics with OS-level counters (context switches, cache misses). Safe Fallbacks : Guaranteed execution on CPU if specialized hardware is unavailable.","title":"\ud83d\ude80 Key Features"},{"location":"#architecture-overview","text":"Component Language Responsibility Core API Python User interface, experiment orchestration, visualization. System Agent Rust Safe threading, OS metric collection, background daemons. Kernels C++ SIMD-optimized math operations, hardware-specific acceleration. Device Abstraction C Low-level hardware query interface.","title":"\ud83c\udfd7 Architecture Overview"},{"location":"#supported-platforms","text":"Linux (Primary, extensive support) macOS (Apple Silicon support via MPS) Windows (Experimental via WSL2)","title":"\ud83d\udcbb Supported Platforms"},{"location":"#documentation","text":"Architecture Overview : High-level system design. CPU Execution : Deep dive into memory & threading. GPU Comparison : When to use ai_optix vs. PyTorch for GPU. Performance Notes : Bottlenecks and optimization guide.","title":"\ud83d\udcda Documentation"},{"location":"#quick-start","text":"","title":"\u26a1 Quick Start"},{"location":"#installation-pip","text":"We provide an automated setup script that handles: - Python 3.11 Virtual Environment creation - CPU vs CUDA Runtime detection - PyTorch version selection # Clone the repository git clone https://github.com/ai-foundation-software/ai-optix.git cd ai-optix # Run the setup script ./scripts/setup.sh # Activate the environment source .venv/bin/activate","title":"Installation (Pip)"},{"location":"#running-a-benchmark","text":"# Run the Reference MatMul Benchmark python -m ai_optix.benchmarks.mebs.matmul_benchmark","title":"Running a Benchmark"},{"location":"#python-api-example","text":"from ai_optix.benchmarks.mebs import BenchmarkRunner, BenchmarkConfig import torch def my_model_op(): # Your model code here pass config = BenchmarkConfig(name=\"MyModel\", warmup_iters=10, measure_iters=50) runner = BenchmarkRunner(config) runner.run(my_model_op)","title":"Python API Example"},{"location":"#safety-philosophy","text":"AI Optix adheres to strict principles: 1. Never optimize without measuring. 2. Never sacrifice precision for speed (unless explicitly requested via quantization levels). 3. Always report the baseline.","title":"\ud83d\udee1 Safety Philosophy"},{"location":"#roadmap","text":"[x] Initial Hybrid Architecture (Python/Rust/C++) [x] Model Efficiency Benchmark Suite (MEBS) [ ] Automated Kernel Tuning [ ] Distributed Training Profiling [ ] Web-based Visualization Dashboard","title":"\ud83d\uddfa Roadmap"},{"location":"#contributing","text":"We welcome contributions! Please see CONTRIBUTING.md for details on our code style and development workflow.","title":"\ud83e\udd1d Contributing"},{"location":"#license","text":"This project is licensed under the MIT License - see the LICENSE file for details.","title":"\ud83d\udcc4 License"},{"location":"ARCHITECTURE/","text":"AI Optix Architecture This document outlines the high-level architecture of ai_optix , a hybrid Python/Rust/C++ library designed for high-performance AI optimization on CPU. \ud83c\udfd7 High-Level Overview ai_optix follows a \"sandwich\" architecture similar to modern high-performance libraries (like polars or pydantic-core ), but with a dedicated C++ kernel layer for raw computational throughput. graph TD User[User Python Script] --> API[ai_optix (Python)] API --> Rust[Rust Extension (_core)] subgraph \"Native Execution Layer\" Rust --> |FFI / Unsafe| Cpp[C++ Kernels] Cpp --> |OpenMP| CPU[CPU Cores] Cpp --> |CUDA (Optional)| GPU[NVIDIA GPU] end subgraph \"Memory Management\" Rust -- Borrow --> PyMem[Python Memory (NumPy)] Cpp -- Pointer --> PyMem end \ud83d\udcc2 Directory Structure & Responsibilities Path Responsibility Language ai_optix/ Public API Surface . Pure Python. Handles type validation, high-level orchestration, and user-facing errors. Python src/rust/ System Glue & Safety . Handles Python C-API interaction (via PyO3), memory safety, concurrent data loading, and orchestration of C++ kernels. Rust src/cpp/ Computational Core . Raw bare-metal loops, OpenMP parallelization, and SIMD intrinsics. No Python awareness here. C++ cmake/ Build Logic . Configuration for compiling C++ kernels across Linux/macOS/Windows. CMake \ud83d\udd0c The Interface Boundary Python \u2194 Rust ( maturin + pyo3 ) We use maturin to build the Rust extension. The Rust layer ( src/rust/src/lib.rs ) exposes classes like Optimizer directly to Python. - Zero-Copy : We aim to pass NumPy array pointers directly to Rust/C++ without copying data, using the Buffer Protocol. Rust \u2194 C++ ( cc / cmake ) Rust links against the static or shared C++ library ( kernels_cpu ). - bindgen (conceptual): Rust defines extern \"C\" blocks to call C++ functions. - Safety : Rust encapsulates the unsafe calls to C++, ensuring that pointers are valid and arrays are locked before C++ touches them. \ud83d\udee0 Build System The build is a hybrid process: 1. Pip/Build : Invokes maturin . 2. Maturin : Compiles the Rust crate. 3. Rust Build Script ( build.rs ) : Invokes cmake . 4. CMake : Compiles src/cpp/kernels.cpp into libkernels_cpu.a . 5. Linker : Statically links C++ kernels into the Rust extension, which is loaded by Python.","title":"Architecture"},{"location":"ARCHITECTURE/#ai-optix-architecture","text":"This document outlines the high-level architecture of ai_optix , a hybrid Python/Rust/C++ library designed for high-performance AI optimization on CPU.","title":"AI Optix Architecture"},{"location":"ARCHITECTURE/#high-level-overview","text":"ai_optix follows a \"sandwich\" architecture similar to modern high-performance libraries (like polars or pydantic-core ), but with a dedicated C++ kernel layer for raw computational throughput. graph TD User[User Python Script] --> API[ai_optix (Python)] API --> Rust[Rust Extension (_core)] subgraph \"Native Execution Layer\" Rust --> |FFI / Unsafe| Cpp[C++ Kernels] Cpp --> |OpenMP| CPU[CPU Cores] Cpp --> |CUDA (Optional)| GPU[NVIDIA GPU] end subgraph \"Memory Management\" Rust -- Borrow --> PyMem[Python Memory (NumPy)] Cpp -- Pointer --> PyMem end","title":"\ud83c\udfd7 High-Level Overview"},{"location":"ARCHITECTURE/#directory-structure-responsibilities","text":"Path Responsibility Language ai_optix/ Public API Surface . Pure Python. Handles type validation, high-level orchestration, and user-facing errors. Python src/rust/ System Glue & Safety . Handles Python C-API interaction (via PyO3), memory safety, concurrent data loading, and orchestration of C++ kernels. Rust src/cpp/ Computational Core . Raw bare-metal loops, OpenMP parallelization, and SIMD intrinsics. No Python awareness here. C++ cmake/ Build Logic . Configuration for compiling C++ kernels across Linux/macOS/Windows. CMake","title":"\ud83d\udcc2 Directory Structure &amp; Responsibilities"},{"location":"ARCHITECTURE/#the-interface-boundary","text":"","title":"\ud83d\udd0c The Interface Boundary"},{"location":"ARCHITECTURE/#python-rust-maturin-pyo3","text":"We use maturin to build the Rust extension. The Rust layer ( src/rust/src/lib.rs ) exposes classes like Optimizer directly to Python. - Zero-Copy : We aim to pass NumPy array pointers directly to Rust/C++ without copying data, using the Buffer Protocol.","title":"Python \u2194 Rust (maturin + pyo3)"},{"location":"ARCHITECTURE/#rust-c-cc-cmake","text":"Rust links against the static or shared C++ library ( kernels_cpu ). - bindgen (conceptual): Rust defines extern \"C\" blocks to call C++ functions. - Safety : Rust encapsulates the unsafe calls to C++, ensuring that pointers are valid and arrays are locked before C++ touches them.","title":"Rust \u2194 C++ (cc / cmake)"},{"location":"ARCHITECTURE/#build-system","text":"The build is a hybrid process: 1. Pip/Build : Invokes maturin . 2. Maturin : Compiles the Rust crate. 3. Rust Build Script ( build.rs ) : Invokes cmake . 4. CMake : Compiles src/cpp/kernels.cpp into libkernels_cpu.a . 5. Linker : Statically links C++ kernels into the Rust extension, which is loaded by Python.","title":"\ud83d\udee0 Build System"},{"location":"MEBS/","text":"Model Efficiency Benchmark Suite (MEBS) Guide MEBS is the standard benchmarking infrastructure for ai_optix . It ensures that all performance numbers we report are: 1. Correct : Handling GPU synchronization and timer resolution automatically. 2. Reproducible : Enforcing warmup cycles and statistical aggregation. 3. Hardware-Aware : Automatically detecting CUDA, MPS, or CPU environments. \ud83d\ude80 Quick Start Using the @benchmark Decorator The easiest way to measure a function is to decorate it. from ai_optix.mebs import benchmark @benchmark(\"My Operation\", warmup=10, measure=100) def my_op(): # Your code here # If using GPU, PyTorch ops are automatically synchronized by MEBS torch.matmul(a, b) # Running the function triggers the benchmark and returns stats stats = my_op() print(f\"Mean Latency: {stats.mean_ms:.4f} ms\") Manual Runner API For more control (e.g., changing config dynamically), use LatencyRunner . from ai_optix.mebs import BenchmarkConfig, LatencyRunner def my_op(): pass config = BenchmarkConfig( name=\"Dynamic Bench\", warmup_iters=5, measure_iters=20, device=\"cuda\" # Force a specific device ) runner = LatencyRunner(config) stats = runner.run(my_op) \ud83d\udcca Understanding Requirements When contributing to ai_optix , you MUST use MEBS for any performance claims. Why not time.time() ? Standard Python timers do not account for: - Asynchronous GPU Execution : GPU kernels return control to Python immediately. time.time() measures launch time, not execution time. MEBS uses torch.cuda.Event for correct timing. - Cold Start : The first few iterations of any ML op include allocator overhead, kernel compilation, or cache warming. MEBS enforces warmup. - OS Noise : A single run can be affected by context switches. MEBS reports P50/P99 to show stability. \ud83d\udee0 Advanced Usage Throughput Benchmarking For extremely fast operations (nanoseconds), per-iteration timing overhead is too high. Use ThroughputRunner to measure a batch. from ai_optix.mebs.runners.throughput import ThroughputRunner, ThroughputConfig config = ThroughputConfig(\"Tiny Op\", measure_iters=10000) runner = ThroughputRunner(config) stats = runner.run(tiny_op) print(f\"Throughput: {stats.throughput_per_sec:.2f} ops/sec\")","title":"MEBS"},{"location":"MEBS/#model-efficiency-benchmark-suite-mebs-guide","text":"MEBS is the standard benchmarking infrastructure for ai_optix . It ensures that all performance numbers we report are: 1. Correct : Handling GPU synchronization and timer resolution automatically. 2. Reproducible : Enforcing warmup cycles and statistical aggregation. 3. Hardware-Aware : Automatically detecting CUDA, MPS, or CPU environments.","title":"Model Efficiency Benchmark Suite (MEBS) Guide"},{"location":"MEBS/#quick-start","text":"","title":"\ud83d\ude80 Quick Start"},{"location":"MEBS/#using-the-benchmark-decorator","text":"The easiest way to measure a function is to decorate it. from ai_optix.mebs import benchmark @benchmark(\"My Operation\", warmup=10, measure=100) def my_op(): # Your code here # If using GPU, PyTorch ops are automatically synchronized by MEBS torch.matmul(a, b) # Running the function triggers the benchmark and returns stats stats = my_op() print(f\"Mean Latency: {stats.mean_ms:.4f} ms\")","title":"Using the @benchmark Decorator"},{"location":"MEBS/#manual-runner-api","text":"For more control (e.g., changing config dynamically), use LatencyRunner . from ai_optix.mebs import BenchmarkConfig, LatencyRunner def my_op(): pass config = BenchmarkConfig( name=\"Dynamic Bench\", warmup_iters=5, measure_iters=20, device=\"cuda\" # Force a specific device ) runner = LatencyRunner(config) stats = runner.run(my_op)","title":"Manual Runner API"},{"location":"MEBS/#understanding-requirements","text":"When contributing to ai_optix , you MUST use MEBS for any performance claims.","title":"\ud83d\udcca Understanding Requirements"},{"location":"MEBS/#why-not-timetime","text":"Standard Python timers do not account for: - Asynchronous GPU Execution : GPU kernels return control to Python immediately. time.time() measures launch time, not execution time. MEBS uses torch.cuda.Event for correct timing. - Cold Start : The first few iterations of any ML op include allocator overhead, kernel compilation, or cache warming. MEBS enforces warmup. - OS Noise : A single run can be affected by context switches. MEBS reports P50/P99 to show stability.","title":"Why not time.time()?"},{"location":"MEBS/#advanced-usage","text":"","title":"\ud83d\udee0 Advanced Usage"},{"location":"MEBS/#throughput-benchmarking","text":"For extremely fast operations (nanoseconds), per-iteration timing overhead is too high. Use ThroughputRunner to measure a batch. from ai_optix.mebs.runners.throughput import ThroughputRunner, ThroughputConfig config = ThroughputConfig(\"Tiny Op\", measure_iters=10000) runner = ThroughputRunner(config) stats = runner.run(tiny_op) print(f\"Throughput: {stats.throughput_per_sec:.2f} ops/sec\")","title":"Throughput Benchmarking"},{"location":"PROJECT_AUDIT/","text":"Project Structure Audit: AI Optix Date: 2025-12-16 Auditor: AI Systems Architect Scope: Full Repository Review 1. Executive Summary The AI Optix repository demonstrates a high level of professionalism . The hybrid Python/Rust/C++ architecture is correctly scaffolded using modern tooling ( maturin , pyproject.toml ). The separation of concerns between high-level orchestration (Python) and low-level execution (Rust/C++) is clear. However, several standard open-source files are missing (LICENSE, CI/CD), and the build system has potential redundancies. 2. Directory Structure Review \u2705 Strengths src/rust & src/cpp : Clean separation of native code. ai_optix/benchmarks/mebs : Excellent modularity for benchmarks. pyproject.toml : Uses modern standard (PEP 621 compliant). \u26a0\ufe0f Observations / Risks CMakeLists.txt at root : It is unclear if this is the primary entry point for C++ or if src/rust/build.rs drives CMake. Recommendation: Clarify if maturin drives everything or if manual C++ build is needed. Missing .github/workflows : No CI/CD is currently active. Missing LICENSE : Referenced in documentation but likely missing from disk. 3. Build System Sanity Python : pyproject.toml correctly defines maturin as the build backend. Rust : src/rust/Cargo.toml correctly defines a cdylib extension. C++ : Integration via cc or cmake crate in build.rs is the standard safe pattern. Verdict : The build system is sane and follows best practices for PyO3/Maturin projects. 4. Improvements & Recommendations A. Critical (Pre-Release) Add LICENSE file : MIT or Apache 2.0. Add .gitignore : Ensure target/ , __pycache__/ , .venv/ are excluded. CI/CD : Add GitHub Action to build wheels on Linux/Mac/Windows. B. Scalability Namespace Packages : Consider if ai_optix should be a namespace package if plugins are expected. Type Stubs : Add *.pyi files for the Rust extension to support IDE autocompletion (since _core is binary). 5. Recommended CI/CD Workflow Create .github/workflows/pipeline.yml : name: CI on: [push, pull_request] jobs: test: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: dtolnay/rust-toolchain@stable - uses: actions/setup-python@v4 with: python-version: '3.10' - name: Build run: pip install maturin && maturin develop - name: Test run: pip install pytest && pytest 6. Conclusion AI Optix is structurally sound. With the addition of CI/CD and License files, it is ready for public release. The codebase reflects a \"Correctness First\" philosophy in its layout by prioritizing clear module boundaries.","title":"Project Audit"},{"location":"PROJECT_AUDIT/#project-structure-audit-ai-optix","text":"Date: 2025-12-16 Auditor: AI Systems Architect Scope: Full Repository Review","title":"Project Structure Audit: AI Optix"},{"location":"PROJECT_AUDIT/#1-executive-summary","text":"The AI Optix repository demonstrates a high level of professionalism . The hybrid Python/Rust/C++ architecture is correctly scaffolded using modern tooling ( maturin , pyproject.toml ). The separation of concerns between high-level orchestration (Python) and low-level execution (Rust/C++) is clear. However, several standard open-source files are missing (LICENSE, CI/CD), and the build system has potential redundancies.","title":"1. Executive Summary"},{"location":"PROJECT_AUDIT/#2-directory-structure-review","text":"","title":"2. Directory Structure Review"},{"location":"PROJECT_AUDIT/#strengths","text":"src/rust & src/cpp : Clean separation of native code. ai_optix/benchmarks/mebs : Excellent modularity for benchmarks. pyproject.toml : Uses modern standard (PEP 621 compliant).","title":"\u2705 Strengths"},{"location":"PROJECT_AUDIT/#observations-risks","text":"CMakeLists.txt at root : It is unclear if this is the primary entry point for C++ or if src/rust/build.rs drives CMake. Recommendation: Clarify if maturin drives everything or if manual C++ build is needed. Missing .github/workflows : No CI/CD is currently active. Missing LICENSE : Referenced in documentation but likely missing from disk.","title":"\u26a0\ufe0f Observations / Risks"},{"location":"PROJECT_AUDIT/#3-build-system-sanity","text":"Python : pyproject.toml correctly defines maturin as the build backend. Rust : src/rust/Cargo.toml correctly defines a cdylib extension. C++ : Integration via cc or cmake crate in build.rs is the standard safe pattern. Verdict : The build system is sane and follows best practices for PyO3/Maturin projects.","title":"3. Build System Sanity"},{"location":"PROJECT_AUDIT/#4-improvements-recommendations","text":"","title":"4. Improvements &amp; Recommendations"},{"location":"PROJECT_AUDIT/#a-critical-pre-release","text":"Add LICENSE file : MIT or Apache 2.0. Add .gitignore : Ensure target/ , __pycache__/ , .venv/ are excluded. CI/CD : Add GitHub Action to build wheels on Linux/Mac/Windows.","title":"A. Critical (Pre-Release)"},{"location":"PROJECT_AUDIT/#b-scalability","text":"Namespace Packages : Consider if ai_optix should be a namespace package if plugins are expected. Type Stubs : Add *.pyi files for the Rust extension to support IDE autocompletion (since _core is binary).","title":"B. Scalability"},{"location":"PROJECT_AUDIT/#5-recommended-cicd-workflow","text":"Create .github/workflows/pipeline.yml : name: CI on: [push, pull_request] jobs: test: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: dtolnay/rust-toolchain@stable - uses: actions/setup-python@v4 with: python-version: '3.10' - name: Build run: pip install maturin && maturin develop - name: Test run: pip install pytest && pytest","title":"5. Recommended CI/CD Workflow"},{"location":"PROJECT_AUDIT/#6-conclusion","text":"AI Optix is structurally sound. With the addition of CI/CD and License files, it is ready for public release. The codebase reflects a \"Correctness First\" philosophy in its layout by prioritizing clear module boundaries.","title":"6. Conclusion"}]}